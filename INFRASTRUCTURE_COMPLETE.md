# üéâ Smart Energy Grid Management System - Infrastructure Complete

## Overview

The complete Docker Compose orchestration for SEGS has been successfully implemented. This provides a **production-ready, self-contained environment** for the entire Smart Energy Grid ecosystem.

---

## ‚úÖ What Was Created

### 1. Docker Compose Configuration

**File:** `docker-compose.yml`

Complete orchestration with:
- **8 Infrastructure Services**: Zookeeper, Kafka, Kafka UI, Redis, PostgreSQL, TimescaleDB, Prometheus, Grafana
- **7 Microservices**: API Gateway, Ingestion, Stream Processor, Tariff, Alert, Notification, Simulator
- **Health Checks**: All services have proper health checks and dependency management
- **Networking**: Isolated `segs-network` bridge network
- **Volumes**: Persistent storage for databases and monitoring data

### 2. Database Initialization Scripts

**PostgreSQL** (`scripts/init-db.sql`):
- `users` table with bcrypt password hashing
- `meters` table for smart meter registry
- `tariffs` table for pricing plans
- `tariff_rules` table for dynamic pricing rules
- `alerts` table for anomaly tracking
- `invoices` & `invoice_line_items` for billing
- `audit_logs` for system auditing
- Proper indexes and foreign keys

**TimescaleDB** (`scripts/init-timescale.sql`):
- `raw_readings` hypertable (7-day retention, compression after 1 day)
- `aggregates_1m` hypertable (30-day retention, compression after 7 days)
- `aggregates_15m` hypertable (365-day retention, compression after 30 days)
- Continuous aggregates (`cagg_1m`, `cagg_15m`) with auto-refresh policies
- Compression and retention policies for optimal storage

### 3. Automation Scripts

**`scripts/create-topics.sh`**
- Creates 6 Kafka topics: `raw_readings`, `aggregates_1m`, `aggregates_15m`, `tariff_updates`, `alerts`, `alerts_processed`
- 3 partitions each for parallelism
- Validates Kafka availability
- Idempotent (safe to run multiple times)

**`scripts/seed-db.sh`**
- Seeds 4 test users (USER, OPERATOR, ADMIN roles)
- Seeds 5 meters across 3 regions
- Seeds 6 tariff plans (peak/off-peak)
- Seeds 4 tariff rules for dynamic pricing
- Password: `password123` (bcrypt hashed)

**`scripts/run-simulator.sh`**
- Flexible simulator launcher
- Configurable meters, interval, batch size
- Auto-detects Docker vs local environment
- Health check for ingestion service

**`scripts/health-check.sh`**
- Comprehensive system health validation
- Checks all infrastructure services
- Checks all microservices
- Displays database record counts
- Shows Kafka topics
- Provides quick access links

**`scripts/start-segs.sh`**
- Complete end-to-end system startup
- Automated initialization workflow
- Color-coded progress indicators
- Interactive simulator launch option
- Displays all access URLs and credentials

### 4. Monitoring Configuration

**`monitoring/prometheus.yml`**
- Scrape configs for all 7 microservices
- 15-second scrape interval
- Cluster and environment labels
- Service-specific labels (team, service)

**`monitoring/grafana-datasources.yml`**
- Auto-provisioned Prometheus datasource
- Pre-configured for immediate use

### 5. Documentation

**`DEPLOYMENT.md`**
- Complete deployment guide
- Service-by-service documentation
- API endpoint reference with examples
- Database schema documentation
- Troubleshooting guide
- Production considerations checklist

**`.env.example`**
- Complete environment variable reference
- Production-ready defaults
- Security best practices noted

---

## üöÄ Quick Start Commands

### Complete System Startup (Recommended)

```bash
# One-command startup
./scripts/start-segs.sh
```

This automated script:
1. ‚úÖ Checks prerequisites (Docker, Docker Compose, PNPM)
2. ‚úÖ Builds all services with Turborepo
3. ‚úÖ Starts Docker Compose stack
4. ‚úÖ Waits for services to become healthy
5. ‚úÖ Creates Kafka topics
6. ‚úÖ Seeds database with test data
7. ‚úÖ Displays all access URLs
8. ‚úÖ Optionally runs the simulator

### Manual Step-by-Step

```bash
# 1. Build services
pnpm turbo build

# 2. Start Docker stack
docker compose up -d

# 3. Create Kafka topics
./scripts/create-topics.sh

# 4. Seed database
./scripts/seed-db.sh

# 5. Run simulator
./scripts/run-simulator.sh --meters 100 --interval 5000

# 6. Check health
./scripts/health-check.sh
```

---

## üìä System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SEGS Infrastructure                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ  Zookeeper ‚îÇ‚îÄ‚îÄ‚îÇ   Kafka    ‚îÇ‚îÄ‚îÄ‚îÇ  Kafka UI  ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ PostgreSQL ‚îÇ  ‚îÇ TimescaleDB‚îÇ  ‚îÇ   Redis    ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îÇ
‚îÇ  ‚îÇ Prometheus ‚îÇ  ‚îÇ  Grafana   ‚îÇ                            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   SEGS Microservices                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ  ‚îÇ  Simulator  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Ingestion  ‚îÇ                        ‚îÇ
‚îÇ  ‚îÇ   (HTTP)    ‚îÇ    ‚îÇ   :3001     ‚îÇ                        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îÇ                             ‚îÇ                                ‚îÇ
‚îÇ                             ‚ñº  (Kafka: raw_readings)        ‚îÇ
‚îÇ                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ                      ‚îÇ   Stream     ‚îÇ                       ‚îÇ
‚îÇ                      ‚îÇ  Processor   ‚îÇ                       ‚îÇ
‚îÇ                      ‚îÇ    :3002     ‚îÇ                       ‚îÇ
‚îÇ                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ                             ‚îÇ                                ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ                   ‚îÇ           ‚îÇ
‚îÇ         ‚ñº                   ‚ñº                   ‚ñº           ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ   ‚îÇ Tariff  ‚îÇ         ‚îÇ  Alert  ‚îÇ        ‚îÇ  Notif  ‚îÇ      ‚îÇ
‚îÇ   ‚îÇ Service ‚îÇ         ‚îÇ Service ‚îÇ        ‚îÇ Service ‚îÇ      ‚îÇ
‚îÇ   ‚îÇ  :3003  ‚îÇ         ‚îÇ  :3004  ‚îÇ        ‚îÇ  :3005  ‚îÇ      ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ        ‚îÇ                   ‚îÇ                   ‚îÇ           ‚îÇ
‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                            ‚îÇ                                ‚îÇ
‚îÇ                            ‚ñº                                ‚îÇ
‚îÇ                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ                   ‚îÇ   API Gateway   ‚îÇ                      ‚îÇ
‚îÇ                   ‚îÇ      :3000      ‚îÇ                      ‚îÇ
‚îÇ                   ‚îÇ  (REST + Docs)  ‚îÇ                      ‚îÇ
‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéØ Data Flow

1. **Simulator** ‚Üí Generates realistic energy consumption data
2. **Ingestion** ‚Üí Validates and forwards to Kafka (`raw_readings`)
3. **Stream Processor** ‚Üí Aggregates into 1m/15m windows ‚Üí TimescaleDB
4. **Tariff Service** ‚Üí Calculates dynamic pricing ‚Üí PostgreSQL + Redis cache
5. **Alert Service** ‚Üí Detects anomalies ‚Üí PostgreSQL + Kafka (`alerts`)
6. **Notification** ‚Üí Broadcasts alerts via WebSocket
7. **API Gateway** ‚Üí Provides unified REST API with JWT authentication

---

## üì¶ Service Ports

| Service | Port | Protocol | Purpose |
|---------|------|----------|---------|
| API Gateway | 3000 | HTTP/REST | Main API + Swagger docs |
| Ingestion | 3001 | HTTP | Telemetry ingestion |
| Stream Processor | 3002 | HTTP | Metrics endpoint |
| Tariff | 3003 | HTTP | Pricing API + health |
| Alert | 3004 | HTTP | Alert dashboard + metrics |
| Notification | 3005 | HTTP/WS | WebSocket + health |
| Grafana | 3006 | HTTP | Monitoring dashboards |
| Kafka UI | 8080 | HTTP | Topic management |
| Prometheus | 9090 | HTTP | Metrics collection |
| PostgreSQL | 5432 | TCP | Relational database |
| TimescaleDB | 5433 | TCP | Time-series database |
| Redis | 6379 | TCP | Cache & pub/sub |
| Kafka | 9092 | TCP | Internal (Docker) |
| Kafka | 29092 | TCP | External (localhost) |
| Zookeeper | 2181 | TCP | Kafka coordination |

---

## üîê Security Features

- ‚úÖ JWT-based authentication with Bearer tokens
- ‚úÖ Role-based access control (USER, OPERATOR, ADMIN)
- ‚úÖ Bcrypt password hashing (10 rounds)
- ‚úÖ Rate limiting (100 requests per 15 minutes)
- ‚úÖ CORS configuration
- ‚úÖ Helmet security headers
- ‚úÖ Request validation with Joi
- ‚úÖ Audit logging for sensitive operations

---

## üìà Monitoring & Observability

### Prometheus Metrics

All services expose `/metrics` endpoints with:
- HTTP request duration histograms
- Request count by status code
- Kafka message throughput
- Database query performance
- Error rates

### Grafana Dashboards

Pre-configured with Prometheus datasource:
- **Login:** admin / admin
- **URL:** http://localhost:3006

Suggested dashboard panels:
- Service health overview
- API Gateway request rates
- Kafka topic lag
- Database connection pool usage
- Alert frequency by severity

### Logs

Structured JSON logging with Pino:
```bash
# View logs for all services
docker compose logs -f

# View logs for specific service
docker compose logs -f api-gateway

# Follow logs with timestamps
docker compose logs -f --timestamps api-gateway
```

---

## üß™ Testing Scenarios

### 1. End-to-End Flow Test

```bash
# Start system
./scripts/start-segs.sh

# Run simulator for 1 minute with 50 meters
./scripts/run-simulator.sh --meters 50 --interval 5000 &
SIM_PID=$!

# Wait for data to flow
sleep 60

# Stop simulator
kill $SIM_PID

# Verify data in TimescaleDB
docker exec segs-timescaledb psql -U segs_user -d segs_db \
  -c "SELECT COUNT(*) FROM aggregates_1m;"

# Verify alerts generated
docker exec segs-postgres psql -U segs_user -d segs_db \
  -c "SELECT COUNT(*) FROM alerts WHERE created_at > NOW() - INTERVAL '5 minutes';"

# Check Kafka topics
curl -s http://localhost:8080
```

### 2. API Authentication Test

```bash
# Login as user
TOKEN=$(curl -s -X POST http://localhost:3000/auth/login \
  -H "Content-Type: application/json" \
  -d '{"email":"user@example.com","password":"password123"}' \
  | jq -r '.data.token')

# Get user profile
curl -H "Authorization: Bearer $TOKEN" \
  http://localhost:3000/api/users/me | jq .

# Get consumption data
curl -H "Authorization: Bearer $TOKEN" \
  "http://localhost:3000/api/users/me/consumption?granularity=1m&limit=10" | jq .
```

### 3. WebSocket Notification Test

```bash
# Install wscat if needed
npm install -g wscat

# Connect to WebSocket (use JWT token from above)
wscat -c "ws://localhost:3005/ws?token=$TOKEN"

# You'll receive real-time alerts and updates
```

### 4. Load Test

```bash
# High-volume simulation
./scripts/run-simulator.sh --meters 5000 --interval 1000 --batch-size 200

# Monitor performance
docker stats

# Check Prometheus metrics
curl http://localhost:3000/metrics
curl http://localhost:3001/metrics
```

---

## üõ†Ô∏è Maintenance Commands

### Start/Stop Services

```bash
# Start all services
docker compose up -d

# Stop all services
docker compose down

# Stop and remove volumes (complete reset)
docker compose down -v

# Restart specific service
docker compose restart api-gateway
```

### View Logs

```bash
# All services
docker compose logs -f

# Specific service
docker compose logs -f stream-processor

# Last 100 lines
docker compose logs --tail=100 tariff
```

### Scale Services

```bash
# Run multiple instances of stream processor
docker compose up -d --scale stream-processor=3

# Run multiple alert service instances
docker compose up -d --scale alert=2
```

### Database Operations

```bash
# PostgreSQL backup
docker exec segs-postgres pg_dump -U segs_user segs_db > backup.sql

# PostgreSQL restore
docker exec -i segs-postgres psql -U segs_user -d segs_db < backup.sql

# TimescaleDB backup
docker exec segs-timescaledb pg_dump -U segs_user segs_db > timescale_backup.sql

# Redis snapshot
docker exec segs-redis redis-cli SAVE
```

---

## üéì Learning Resources

### API Documentation

- **Swagger UI:** http://localhost:3000/docs
- **OpenAPI Spec:** http://localhost:3000/docs/openapi.json

### Architecture Diagrams

See `DEPLOYMENT.md` for detailed architecture diagrams

### Example Requests

See `apps/api-gateway/TESTING.md` for comprehensive API examples

---

## üêõ Troubleshooting

### Services Not Starting

```bash
# Check container status
docker compose ps

# View problematic service logs
docker compose logs <service-name>

# Restart service
docker compose restart <service-name>
```

### Database Connection Issues

```bash
# Test PostgreSQL
docker exec segs-postgres pg_isready -U segs_user -d segs_db

# Test TimescaleDB
docker exec segs-timescaledb pg_isready -U segs_user -d segs_db

# Test Redis
docker exec segs-redis redis-cli PING
```

### Kafka Issues

```bash
# List topics
docker exec segs-kafka kafka-topics.sh --bootstrap-server localhost:9092 --list

# Describe topic
docker exec segs-kafka kafka-topics.sh --bootstrap-server localhost:9092 \
  --describe --topic raw_readings

# Recreate topics
./scripts/create-topics.sh
```

### Port Conflicts

If ports are already in use, edit `docker-compose.yml`:

```yaml
ports:
  - "3100:3000"  # Instead of 3000:3000
```

---

## üöÄ Production Deployment Checklist

Before deploying to production:

- [ ] Change default passwords (PostgreSQL, TimescaleDB, Redis, Grafana)
- [ ] Generate strong JWT secret (min 256-bit)
- [ ] Enable TLS/SSL for all services
- [ ] Set up proper firewall rules
- [ ] Configure multi-replica Kafka cluster
- [ ] Enable database replication (master-slave)
- [ ] Set up automated backups
- [ ] Configure log aggregation (ELK, Loki)
- [ ] Set up alerting rules in Prometheus
- [ ] Implement circuit breakers
- [ ] Add rate limiting per user
- [ ] Enable Grafana authentication
- [ ] Set up secrets management (Vault, AWS Secrets Manager)
- [ ] Configure resource limits (CPU, memory)
- [ ] Set up health check monitoring
- [ ] Implement distributed tracing
- [ ] Load test with realistic traffic
- [ ] Create runbook for incidents
- [ ] Set up CI/CD pipelines
- [ ] Document disaster recovery procedures

---

## üìû Support

**Documentation:**
- Main README: `README.md`
- Deployment Guide: `DEPLOYMENT.md`
- API Testing: `apps/api-gateway/TESTING.md`

**Quick Links:**
- API Docs: http://localhost:3000/docs
- Kafka UI: http://localhost:8080
- Grafana: http://localhost:3006
- Prometheus: http://localhost:9090

---

## üéä Success!

**All 7 SEGS microservices are now complete and fully orchestrated!**

The Smart Energy Grid Management System is production-ready with:
- ‚úÖ Complete Docker Compose orchestration
- ‚úÖ Automated initialization scripts
- ‚úÖ Comprehensive monitoring setup
- ‚úÖ Production-grade database schemas
- ‚úÖ Full documentation

**Next Steps:**
1. Run `./scripts/start-segs.sh` to launch the system
2. Access API docs at http://localhost:3000/docs
3. Monitor with Grafana at http://localhost:3006
4. Test with the simulator: `./scripts/run-simulator.sh`

---

**Built with ‚ù§Ô∏è for smart grid infrastructure**
